{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "20e8013a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import string\n",
    "import time\n",
    "import re\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from pickle import dump, load\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torchvision.models as models\n",
    "from torchvision.models import ResNet50_Weights\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "361f7662",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self, min_freq=5):\n",
    "        self.min_freq = min_freq\n",
    "        self.itos = {0:\"pad\", 1:\"startofseq\", 2:\"endofseq\", 3:\"unk\"}\n",
    "        self.stoi = {v:k for k,v in self.itos.items()}\n",
    "        self.index = 4\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "    \n",
    "    def tokenizer(self, text):\n",
    "        text = text.lower()\n",
    "        tokens = re.findall(r\"\\w+\", text) #splits the text into tokens based on punctuation\n",
    "        return tokens\n",
    "\n",
    "    def build_vocab(self, sentence_list):\n",
    "        count = {}\n",
    "        for sentence in sentence_list:\n",
    "            tokens = self.tokenizer(sentence)\n",
    "            for token in tokens:\n",
    "                if token not in count:\n",
    "                    count[token] = 1\n",
    "                else:\n",
    "                    count[token] += 1\n",
    "        \n",
    "        for token, freq in count.items():\n",
    "            # print(f\"{token}:{freq}\")\n",
    "            if freq>=self.min_freq:\n",
    "                #if freq>=min_freq then add it to vocab\n",
    "                self.itos[self.index] = token\n",
    "                self.stoi[token] = self.index\n",
    "                self.index += 1\n",
    "    \n",
    "    def change_to_nums(self, text):\n",
    "        tokens = self.tokenizer(text)\n",
    "        nums = []\n",
    "        for token in tokens:\n",
    "            if token in self.stoi:\n",
    "                nums.append(self.stoi[token])\n",
    "            else:\n",
    "                nums.append(self.stoi[\"unk\"])\n",
    "        return nums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "557fec5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_tokens(filepath):\n",
    "    descriptions = {}\n",
    "    with open(filepath, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        for line in lines[1:]:\n",
    "            line = line.strip()\n",
    "            words = line.split(\".jpg\")\n",
    "            img_id = words[0] + \".jpg\"\n",
    "            caption = words[1:]\n",
    "            cap = \" \".join(caption)\n",
    "\n",
    "            if img_id not in descriptions:\n",
    "                descriptions[img_id] = []\n",
    "            descriptions[img_id].append(cap.strip(',\"'))\n",
    "    \n",
    "    return descriptions\n",
    "\n",
    "# len(parse_tokens(TOKENS_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "3cd93f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, descriptions, vocab, transform=None, train=True):\n",
    "        self.desc = []\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "        imgs=[]\n",
    "        caps=[]\n",
    "\n",
    "        for img_id, captions in descriptions.items():\n",
    "            img_path = os.path.join(IMAGES_DIR, img_id)\n",
    "            if not os.path.isfile(img_path):\n",
    "                continue\n",
    "            else:\n",
    "                for caption in captions:\n",
    "                    imgs.append(img_path)\n",
    "                    caps.append(caption)\n",
    "\n",
    "        itrain, itest, ctrain, ctest = train_test_split(imgs, caps, shuffle=True, test_size=0.2, random_state=SEED)\n",
    "\n",
    "        if train:\n",
    "            for i, img in enumerate(itrain):\n",
    "                self.desc.append((img, ctrain[i]))\n",
    "        else:\n",
    "            for i, img in enumerate(itest):\n",
    "                self.desc.append((img, ctest[i]))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.desc)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, caption = self.desc[idx]\n",
    "        \n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        num_caption = [self.vocab.stoi[\"startofseq\"]] #the first token is startofseq\n",
    "        num_caption += self.vocab.change_to_nums(caption) #we are adding as they r lists\n",
    "        num_caption.append(self.vocab.stoi[\"endofseq\"])\n",
    "\n",
    "        return img, torch.tensor(num_caption, dtype=torch.long)\n",
    "        #the above is a custom tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a7b6dc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is a helper function for the dataloader\n",
    "def padding(data):\n",
    "    #data is what getitem returns so it is a tuple, rows are batched full data\n",
    "    data.sort(key=lambda x:len(x[1]), reverse=True)\n",
    "    imgs = [i[0] for i in data]\n",
    "    captions = [i[1] for i in data]\n",
    "    lens = [len(i) for i in captions]\n",
    "    max_len = max(lens)\n",
    "\n",
    "    padded = torch.zeros(len(captions), max_len, dtype=torch.long) #this is a 2d torch of no.of rows=captions and no.of cols=max_len\n",
    "    for i, caption in enumerate(captions):\n",
    "        end = lens[i]\n",
    "        padded[i, :end] = caption\n",
    "\n",
    "    imgs = torch.stack(imgs, dim=0) #to stack them as [batch, channel, row, col] where batch is the new dim at 0\n",
    "    return imgs, padded, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a344ea5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim):\n",
    "        super().__init__()\n",
    "        model = models.resnet50(weights=ResNet50_Weights.DEFAULT)\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad=True #this is so that we can fine tune the model\n",
    "        #we need to drop the last linear layer and change it to an embedding \n",
    "        modules = list(model.children())[:-1] #dropping the last layer\n",
    "\n",
    "        self.model = nn.Sequential(*modules)\n",
    "        self.fc = nn.Linear(model.fc.in_features, embed_dim) #input is same as fc but output is embedding space\n",
    "        self.batchnorm = nn.BatchNorm1d(embed_dim, momentum=0.01)\n",
    "\n",
    "    def forward(self, imgs):\n",
    "        with torch.no_grad():\n",
    "            features = self.model(imgs)\n",
    "        #these are as [batch size, model.fc.in_features, 1, 1] therefore we need to flatten\n",
    "        features = features.view(features.size(0), -1)\n",
    "        features = self.fc(features) #this means features is [batch, embed dim]\n",
    "        features = self.batchnorm(features)\n",
    "        \n",
    "        return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7f28823d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMDecoder(nn.Module):\n",
    "    def __init__(self, embed_dim, hidden_dim, vocab_size, num_layers=1):\n",
    "        super().__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embed_dim) #it creates a dense vector of size vocab_size and dims of embed_dim\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, features, captions):\n",
    "        captions_in = captions[:, :-1] #captions is the padded tensor\n",
    "        embed = self.embeddings(captions_in)\n",
    "        #lstm needs shape as [batch size, seq length, vocab size]\n",
    "        features = features.unsqueeze(1) #as features are of shape [batchsize, 2048] so we make [batchsize, 1, 2048] as lstm needs this shape\n",
    "        lstm_input = torch.cat((features, embed), dim=1) #we are concating these 2 along dim=1(seq length) so final is (seq length+1) there fore first token is image\n",
    "        outputs, _ = self.lstm(lstm_input)\n",
    "        logits = self.fc(outputs)\n",
    "        \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3b60b9f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FullModel(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, imgs, captions):\n",
    "        features = self.encoder(imgs)\n",
    "        outputs = self.decoder(features, captions)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3142ec7e",
   "metadata": {},
   "source": [
    "## Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5455aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, vocab_size, epoch):\n",
    "    model.train()\n",
    "    total_loss=0\n",
    "    progress = tqdm(dataloader, desc=f\"Epoch {epoch+1}\", unit=\"batch\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for imgs, captions, _lengths in progress:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        captions = captions.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs, captions)\n",
    "        outputs = outputs[:, 1:, :].contiguous().view(-1, vocab_size) #we need to remove first token as it is the image\n",
    "        targets = captions[:, 1:].contiguous().view(-1)\n",
    "        #outputs = [batch * seqlength , vocab size]\n",
    "        #targets = [batch * seqlength] , we are removing first as it is startofseq token\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        progress.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    \n",
    "    avg_loss = total_loss/len(dataloader)\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "    print(f\"Run completed. Time taken: {time_taken:.4f}\")\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d90ac2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, dataloader, criterion, vocab_size):\n",
    "    model.eval()\n",
    "    total_loss=0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, captions, _lengths in dataloader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            captions = captions.to(DEVICE)\n",
    "\n",
    "            outputs = model(imgs, captions)\n",
    "            outputs = outputs[:, 1:, :].contiguous().view((-1, vocab_size))\n",
    "            targets = captions[:, 1:].contiguous().view(-1)\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    avg_loss = total_loss/len(dataloader)\n",
    "    return avg_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "93345b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBED_DIM = 256\n",
    "HIDDEN_DIM = 512\n",
    "LEARNING_RATE = 0.001\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 25\n",
    "MIN_WORD_FREQ = 1\n",
    "SEED = 42\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "NUM_WORKERS = 4\n",
    " \n",
    "IMAGES_DIR = \"data/images\"\n",
    "TOKENS_FILE = \"data/captions.txt\"\n",
    "\n",
    "BEST_CHECKPOINT_PATH = \"checkpoints/best_checkpoint.pth\"\n",
    "FINAL_MODEL_PATH = \"checkpoints/final_model.pth\"\n",
    "VOCAB_PATH = \"vocab/vocab.pkl\"\n",
    " \n",
    "RESUME = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f4cacee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary loaded from vocab/vocab.pkl\n",
      "Vocabulary size: 8492\n"
     ]
    }
   ],
   "source": [
    "if not RESUME: #i.e. if we are starting from scratch\n",
    "    desc = parse_tokens(TOKENS_FILE)\n",
    "    all_captions=[]\n",
    "    # print(desc[list(desc.keys())[0]])\n",
    "    for captions in desc.values():\n",
    "        all_captions.extend(captions)\n",
    "    \n",
    "    # print(len(all_captions))\n",
    "    vocab=Vocabulary(min_freq=1)\n",
    "    vocab.build_vocab(all_captions)\n",
    "    with open(VOCAB_PATH, \"wb\") as f:\n",
    "        dump(vocab, f)\n",
    "    print(f\"Vocabulary saved to {VOCAB_PATH}\")\n",
    "\n",
    "    vocab_size = len(vocab)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")\n",
    "\n",
    "else:\n",
    "    with open(VOCAB_PATH, \"rb\") as f:\n",
    "        vocab = load(f)\n",
    "    print(f\"Vocabulary loaded from {VOCAB_PATH}\")\n",
    "    vocab_size = len(vocab)\n",
    "    print(f\"Vocabulary size: {vocab_size}\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54d26bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224,224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = MyDataset(desc, vocab, transform=transform, train=True)\n",
    "test_dataset = MyDataset(desc, vocab, transform=transform, train=False)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True, num_workers=NUM_WORKERS, collate_fn=padding)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS, collate_fn=padding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "058cf9f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = ResNetEncoder(EMBED_DIM)\n",
    "decoder = LSTMDecoder(EMBED_DIM, HIDDEN_DIM, vocab_size)\n",
    "model = FullModel(encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e9bd94e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 32140396\n",
      "Total trainable parameters: 32140396\n",
      "Resuming from checkpoint: checkpoints/best_checkpoint.pth\n",
      "Resuming at epoch 3, best_val_loss so far: 1.5019\n"
     ]
    }
   ],
   "source": [
    "total_params = sum(param.numel() for param in model.parameters())\n",
    "print(f\"Total parameters: {total_params}\")\n",
    "\n",
    "total_trainable_params = sum(param.numel() for param in model.parameters() if param.requires_grad)\n",
    "print(f\"Total trainable parameters: {total_trainable_params}\")\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"pad\"])\n",
    "#since the model is join of encoder and decoder and we dont need to optimize params of encoder\n",
    "params = list(model.decoder.parameters()) + list(model.encoder.fc.parameters()) + list(model.encoder.batchnorm.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=LEARNING_RATE)\n",
    "\n",
    "start_epoch=0 #this is cuz if we are resuming training from middle we need to know where to start\n",
    "best_val_loss=float(\"inf\")\n",
    "\n",
    "#if we want to resume training\n",
    "if RESUME and os.path.exists(BEST_CHECKPOINT_PATH):\n",
    "    print(f\"Resuming from checkpoint: {BEST_CHECKPOINT_PATH}\")\n",
    "    checkpoint = torch.load(BEST_CHECKPOINT_PATH, map_location=DEVICE)\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    start_epoch = checkpoint[\"epoch\"] + 1\n",
    "    best_val_loss = checkpoint[\"best_val_loss\"]\n",
    "    print(f\"Resuming at epoch {start_epoch+1}, best_val_loss so far: {best_val_loss:.4f}\")\n",
    "\n",
    "elif RESUME:\n",
    "    print(f\"Warning: {BEST_CHECKPOINT_PATH} not found. Will have to start fresh ...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec33192c",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e9e1e851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 1.187027096748352\n"
     ]
    }
   ],
   "source": [
    "#just checking...\n",
    "imgs, captions, lengths = next(iter(train_loader))\n",
    "imgs = imgs.to(DEVICE)\n",
    "captions = captions.to(DEVICE)\n",
    "outputs = model(imgs, captions)\n",
    "outputs = outputs[:, 1:, :].contiguous().view(-1, vocab_size)\n",
    "targets = captions[:, 1:].contiguous().view(-1)\n",
    "loss = criterion(outputs, targets)\n",
    "loss.backward()\n",
    "\n",
    "print(\"Loss:\", loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ac92a992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 505/505 [1:32:55<00:00, 11.04s/batch, loss=1.4077]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed. Time taken: 5575.7826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 3/25] Train Loss: 1.2358 | Val Loss: 1.4714\n",
      "New best model saved at checkpoints/best_checkpoint.pth: 1.4714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4: 100%|██████████| 505/505 [1:36:37<00:00, 11.48s/batch, loss=1.2478]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed. Time taken: 5797.1911\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 4/25] Train Loss: 1.1275 | Val Loss: 1.4488\n",
      "New best model saved at checkpoints/best_checkpoint.pth: 1.4488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5: 100%|██████████| 505/505 [1:33:47<00:00, 11.14s/batch, loss=0.9549]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed. Time taken: 5627.3400\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/25] Train Loss: 1.0239 | Val Loss: 1.4692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6: 100%|██████████| 505/505 [1:31:59<00:00, 10.93s/batch, loss=1.0110]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed. Time taken: 5519.6562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 6/25] Train Loss: 0.9385 | Val Loss: 1.4815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7: 100%|██████████| 505/505 [1:34:32<00:00, 11.23s/batch, loss=0.8409]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed. Time taken: 5672.4881\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 7/25] Train Loss: 0.8551 | Val Loss: 1.4995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8: 100%|██████████| 505/505 [1:36:19<00:00, 11.44s/batch, loss=0.8715]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed. Time taken: 5779.7256\n",
      "[Epoch 8/25] Train Loss: 0.7793 | Val Loss: 1.4961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9: 100%|██████████| 505/505 [1:33:05<00:00, 11.06s/batch, loss=0.5183]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed. Time taken: 5585.8339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 9/25] Train Loss: 0.7164 | Val Loss: 1.5412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10: 100%|██████████| 505/505 [1:33:27<00:00, 11.10s/batch, loss=0.8026]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run completed. Time taken: 5607.7127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 10/25] Train Loss: 0.6527 | Val Loss: 1.5633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11:  30%|██▉       | 151/505 [27:47<1:05:09, 11.04s/batch, loss=0.6807]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training interrupted --> Best model saved at checkpoints/best_checkpoint.pth\n",
      "\n",
      "Final model weights at checkpoints/final_model.pth\n",
      "Best val loss: 1.4488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    for epoch in range(start_epoch, EPOCHS):#this is cuz if we are resuming training from middle we need to know where to start\n",
    "        train_loss = train_one_epoch(model, train_loader, criterion, optimizer, vocab_size, epoch)\n",
    "        val_loss = validation(model, test_loader, criterion, vocab_size)\n",
    "\n",
    "        print(f\"[Epoch {epoch+1}/{EPOCHS}] Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f}\\n\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            checkpoint_dict = {\n",
    "                \"epoch\": epoch,\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "                \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "                \"best_val_loss\": best_val_loss\n",
    "            }\n",
    "\n",
    "            torch.save(checkpoint_dict, BEST_CHECKPOINT_PATH)\n",
    "            print(f\"New best model saved at {BEST_CHECKPOINT_PATH}: {val_loss:.4f}\\n\")\n",
    "\n",
    "            final_checkpoint_dict = {\n",
    "                \"model_state_dict\": model.state_dict(),\n",
    "            }\n",
    "            torch.save(final_checkpoint_dict, FINAL_MODEL_PATH)\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(f\"\\nTraining interrupted --> Best model saved at {BEST_CHECKPOINT_PATH}\\n\")\n",
    "\n",
    "print(f\"\\nFinal model weights at {FINAL_MODEL_PATH}\")\n",
    "print(f\"Best val loss: {best_val_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "img-caption",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
